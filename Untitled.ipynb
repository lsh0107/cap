{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de92c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re\n",
    "import glob, os\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Kkma, Okt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from pyclustering.cluster import kmedoids\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea3ecb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Desktop\\4-2\\cd2\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "print(BASE_DIR)\n",
    "senti = pd.read_csv(BASE_DIR + '/ratings.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d057b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>8963373</td>\n",
       "      <td>포켓 몬스터 짜가 ㅡㅡ;;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>3302770</td>\n",
       "      <td>쓰.레.기</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>5458175</td>\n",
       "      <td>완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>6908648</td>\n",
       "      <td>왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>8548411</td>\n",
       "      <td>포풍저그가나가신다영차영차영차</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1        8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2        4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3        9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4       10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "...          ...                                                ...    ...\n",
       "199995   8963373                                     포켓 몬스터 짜가 ㅡㅡ;;      0\n",
       "199996   3302770                                              쓰.레.기      0\n",
       "199997   5458175                  완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.      0\n",
       "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
       "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31af73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Desktop\\\\4-2\\\\cd2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5dd4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(BASE_DIR + '/test.txt', sep='\\t')\n",
    "df_train = pd.read_csv(BASE_DIR + '/train.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6de8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['document'].nunique(), df_train['label'].nunique()\n",
    "df_train.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108bc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a5a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_22692/1677612632.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_train['document'] = df_train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['document'] = df_train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518e33ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_22692/2419574225.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_train['document'] = df_train['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n"
     ]
    }
   ],
   "source": [
    "df_train['document'] = df_train['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "df_train['document'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4da99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>4221289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>9509970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10147571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>7117896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>6478189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id document  label\n",
       "404   4221289      NaN      0\n",
       "412   9509970      NaN      1\n",
       "470  10147571      NaN      1\n",
       "584   7117896      NaN      0\n",
       "593   6478189      NaN      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[df_train.document.isnull()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4701a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b483f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_22692/2602539837.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_test['document'] = df_test['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_22692/2602539837.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_test['document'] = df_test['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n"
     ]
    }
   ],
   "source": [
    "df_test.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "df_test['document'] = df_test['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "df_test['document'] = df_test['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "df_test['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "df_test = df_test.dropna(how='any') # Null 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55008c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(BASE_DIR + '/train.csv', encoding='utf-8-sig', index=False)\n",
    "df_test.to_csv(BASE_DIR + '/test.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6ced3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(BASE_DIR + '/test.csv')\n",
    "df_train = pd.read_csv(BASE_DIR + '/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37514b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d230b52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9f2eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 145393/145393 [04:48<00:00, 504.48it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for sentence in tqdm(df_train['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e6ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 48852/48852 [01:47<00:00, 456.48it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for sentence in tqdm(df_test['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b215d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "448aa2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 43770\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 24337\n",
      "단어 집합에서 희귀 단어의 비율: 55.60201050948138\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.571480691144935\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4d1078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 19434\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9b3d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af21a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(df_train['label'])\n",
    "y_test = np.array(df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1aa2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0020dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145181\n",
      "145181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adc80ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    count = 0\n",
    "    for sentence in nested_list:\n",
    "        if(len(sentence) <= max_len):\n",
    "            count = count + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6da045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 30 이하인 샘플의 비율: 91.6194267844966\n"
     ]
    }
   ],
   "source": [
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20256dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e35bcda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1814/1815 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8221\n",
      "Epoch 1: val_acc improved from -inf to 0.84420, saving model to best_model.h5\n",
      "1815/1815 [==============================] - 46s 25ms/step - loss: 0.3897 - acc: 0.8221 - val_loss: 0.3528 - val_acc: 0.8442\n",
      "Epoch 2/10\n",
      "1814/1815 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8566\n",
      "Epoch 2: val_acc improved from 0.84420 to 0.85178, saving model to best_model.h5\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.3276 - acc: 0.8566 - val_loss: 0.3426 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "1814/1815 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.8705\n",
      "Epoch 3: val_acc improved from 0.85178 to 0.86131, saving model to best_model.h5\n",
      "1815/1815 [==============================] - 45s 25ms/step - loss: 0.3025 - acc: 0.8705 - val_loss: 0.3259 - val_acc: 0.8613\n",
      "Epoch 4/10\n",
      "1815/1815 [==============================] - ETA: 0s - loss: 0.2834 - acc: 0.8817\n",
      "Epoch 4: val_acc did not improve from 0.86131\n",
      "1815/1815 [==============================] - 46s 25ms/step - loss: 0.2834 - acc: 0.8817 - val_loss: 0.3385 - val_acc: 0.8533\n",
      "Epoch 5/10\n",
      "1814/1815 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.8896\n",
      "Epoch 5: val_acc did not improve from 0.86131\n",
      "1815/1815 [==============================] - 46s 25ms/step - loss: 0.2675 - acc: 0.8896 - val_loss: 0.3263 - val_acc: 0.8605\n",
      "Epoch 6/10\n",
      "1813/1815 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.8975\n",
      "Epoch 6: val_acc did not improve from 0.86131\n",
      "1815/1815 [==============================] - 47s 26ms/step - loss: 0.2528 - acc: 0.8975 - val_loss: 0.3340 - val_acc: 0.8613\n",
      "Epoch 7/10\n",
      "1814/1815 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9043\n",
      "Epoch 7: val_acc did not improve from 0.86131\n",
      "1815/1815 [==============================] - 47s 26ms/step - loss: 0.2378 - acc: 0.9043 - val_loss: 0.3304 - val_acc: 0.8612\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa8ccb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b0cc5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "60.85% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('ㅇㅂ쟈ㅓㅇㄴㅁ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for d in train_x for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "text = nltk.Text(tokens,name='NMSC')#nltk라이브러리를 통해서 텍스트 데이터 나열\n",
    "\n",
    "len(set(text.tokens))#35425개의 고유 텍스트가 존재\n",
    "\n",
    "text.vocab().most_common(20) #vocab().most_common(10) - 텍스트 빈도 상위 10개 보여주기 즉, count_values()를 통해서 내림차순한 것과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6b80095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>[]</td>\n",
       "      <td>나의귀염둥이!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fluffy를 당당히 시켜먹는나이 이제어린이네 20개월</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>['#이모들과', '#모델박찬', '#다음에는']</td>\n",
       "      <td>소개울제빵소 조용히 먹자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>['#찬이가']</td>\n",
       "      <td>있어서 행복하다 근데 뽀뽀했더니 침을 질질흘리고있네... 내입에 다 묻어 부렸다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>['#피부과', '#피부미용에']</td>\n",
       "      <td>갔다왔어요ㅎㅎ 신경써야지요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>['#알맥스랜드⛺️', '#너도', '#사장님']</td>\n",
       "      <td>다리를 꼬을 수 있구나 포스 배부르니 슬슬 잠이 오지???늘어지는 모습</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>['#미리크리스마스🎄', '#오랫만에']</td>\n",
       "      <td>셀카뿅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>['#Newhipseat', '#얼굴에', '#짜증나기']</td>\n",
       "      <td>자꾸 트러블나서 피부과 갔다왔네ㅠ 일보직전</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>['#찬이', '#새옷입고', '#아빠']</td>\n",
       "      <td>할머니 권사취임식날 촬칵 팔아프다 언능 걷자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>yonki88</td>\n",
       "      <td>[]</td>\n",
       "      <td>단둘이 집에남아.. 둘이 스타벅스가는길</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                              tags  \\\n",
       "0           0  yonki88                                []   \n",
       "1           1  yonki88                                []   \n",
       "2           2  yonki88       ['#이모들과', '#모델박찬', '#다음에는']   \n",
       "3           3  yonki88                          ['#찬이가']   \n",
       "4           4  yonki88                ['#피부과', '#피부미용에']   \n",
       "5           5  yonki88       ['#알맥스랜드⛺️', '#너도', '#사장님']   \n",
       "6           6  yonki88            ['#미리크리스마스🎄', '#오랫만에']   \n",
       "7           7  yonki88  ['#Newhipseat', '#얼굴에', '#짜증나기']   \n",
       "8           8  yonki88           ['#찬이', '#새옷입고', '#아빠']   \n",
       "9           9  yonki88                                []   \n",
       "\n",
       "                                         content  \n",
       "0                                       나의귀염둥이!   \n",
       "1                Fluffy를 당당히 시켜먹는나이 이제어린이네 20개월   \n",
       "2                                 소개울제빵소 조용히 먹자   \n",
       "3  있어서 행복하다 근데 뽀뽀했더니 침을 질질흘리고있네... 내입에 다 묻어 부렸다   \n",
       "4                                갔다왔어요ㅎㅎ 신경써야지요   \n",
       "5       다리를 꼬을 수 있구나 포스 배부르니 슬슬 잠이 오지???늘어지는 모습   \n",
       "6                                           셀카뿅   \n",
       "7                       자꾸 트러블나서 피부과 갔다왔네ㅠ 일보직전   \n",
       "8                      할머니 권사취임식날 촬칵 팔아프다 언능 걷자   \n",
       "9                         단둘이 집에남아.. 둘이 스타벅스가는길   "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(BASE_DIR+'/crawlingyonki88.xlsx')\n",
    "df = df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e687b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regular_expression(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 한글 추출 규칙: 띄어 쓰기(1 개)를 포함한 한글\n",
    "    result = hangul.sub('', text)  # 위에 설정한 \"hangul\"규칙을 \"text\"에 적용(.sub)시킴\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe3d6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'있어서 행복하다 근데 뽀뽀했더니 침을 질질흘리고있네 내입에 다 묻어 부렸다 '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_regular_expression(df['content'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "112a617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['뽀뽀', '침', '질질', '입']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()  # 명사 형태소 추출 함수\n",
    "nouns = okt.nouns(apply_regular_expression(df['content'][3]))\n",
    "nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09218c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나의귀염둥이! Fluffy를 당당히 시켜먹는나이 이제어린이네 20개월 소개울제빵소 조용히 먹자 있어서 행복하다 근데 뽀뽀했더니 침을 질질흘리고있네... 내입에 다 묻어 부렸다 갔다왔어요ㅎㅎ 신경써야지요 다리를 꼬을 수 있구나 포스 배부르니 슬슬 잠이 오지???늘어지는 모습 셀카뿅 자꾸 트러블나서 피부과 갔다왔네ㅠ 일보직전 할머니 권사취임식날 촬칵 팔아프다 언능 걷자 단둘이 집에남아.. 둘이 스타벅스가는길 '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\".join(df['content'].tolist())\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "deeddf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나의귀염둥이 를 당당히 시켜먹는나이 이제어린이네 개월 소개울제빵소 조용히 먹자 있어서 행복하다 근데 뽀뽀했더니 침을 질질흘리고있네 내입에 다 묻어 부렸다 갔다왔어요ㅎㅎ 신경써야지요 다리를 꼬을 수 있구나 포스 배부르니 슬슬 잠이 오지늘어지는 모습 셀카뿅 자꾸 트러블나서 피부과 갔다왔네ㅠ 일보직전 할머니 권사취임식날 촬칵 팔아프다 언능 걷자 단둘이 집에남아 둘이 스타벅스가는길 '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_regular_expression(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a53cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '귀염둥이', '를', '어린이', '개월', '소개', '제빵', '소', '뽀뽀', '침', '질질', '입', '신경', '다리', '꼬을', '수', '포스', '슬슬', '잠', '오지', '모습', '셀카', '자꾸', '트러블', '피부', '일보', '직전', '할머니', '권', '취임식', '날', '촬칵', '팔', '단둘', '집', '남아', '둘', '스타벅스']\n"
     ]
    }
   ],
   "source": [
    "nouns = okt.nouns(apply_regular_expression(corpus))\n",
    "print(nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ca47fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "354d101e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 1),\n",
       " ('귀염둥이', 1),\n",
       " ('를', 1),\n",
       " ('어린이', 1),\n",
       " ('개월', 1),\n",
       " ('소개', 1),\n",
       " ('제빵', 1),\n",
       " ('소', 1),\n",
       " ('뽀뽀', 1),\n",
       " ('침', 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d1e7a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('귀염둥이', 1),\n",
       " ('어린이', 1),\n",
       " ('개월', 1),\n",
       " ('소개', 1),\n",
       " ('제빵', 1),\n",
       " ('뽀뽀', 1),\n",
       " ('질질', 1),\n",
       " ('신경', 1),\n",
       " ('다리', 1),\n",
       " ('꼬을', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_counter = Counter({x: counter[x] for x in counter if len(x) > 1})\n",
    "available_counter.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ec0aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['휴'],\n",
       " ['아이구'],\n",
       " ['아이쿠'],\n",
       " ['아이고'],\n",
       " ['어'],\n",
       " ['나'],\n",
       " ['우리'],\n",
       " ['저희'],\n",
       " ['따라'],\n",
       " ['의해'],\n",
       " ['을'],\n",
       " ['를'],\n",
       " ['에'],\n",
       " ['의'],\n",
       " ['가'],\n",
       " ['으로'],\n",
       " ['로'],\n",
       " ['에게'],\n",
       " ['뿐이다'],\n",
       " ['의거하여'],\n",
       " ['근거하여'],\n",
       " ['입각하여'],\n",
       " ['기준으로'],\n",
       " ['예하면'],\n",
       " ['예를 들면'],\n",
       " ['예를 들자면'],\n",
       " ['저'],\n",
       " ['소인'],\n",
       " ['소생'],\n",
       " ['저희'],\n",
       " ['지말고'],\n",
       " ['하지마'],\n",
       " ['하지마라'],\n",
       " ['다른'],\n",
       " ['물론'],\n",
       " ['또한'],\n",
       " ['그리고'],\n",
       " ['비길수 없다'],\n",
       " ['해서는 안된다'],\n",
       " ['뿐만 아니라'],\n",
       " ['만이 아니다'],\n",
       " ['만은 아니다'],\n",
       " ['막론하고'],\n",
       " ['관계없이'],\n",
       " ['그치지 않다'],\n",
       " ['그러나'],\n",
       " ['그런데'],\n",
       " ['하지만'],\n",
       " ['든간에'],\n",
       " ['논하지 않다'],\n",
       " ['따지지 않다'],\n",
       " ['설사'],\n",
       " ['비록'],\n",
       " ['더라도'],\n",
       " ['아니면'],\n",
       " ['만 못하다'],\n",
       " ['하는 편이 낫다'],\n",
       " ['불문하고'],\n",
       " ['향하여'],\n",
       " ['향해서'],\n",
       " ['향하다'],\n",
       " ['쪽으로'],\n",
       " ['틈타'],\n",
       " ['이용하여'],\n",
       " ['타다'],\n",
       " ['오르다'],\n",
       " ['제외하고'],\n",
       " ['이 외에'],\n",
       " ['이 밖에'],\n",
       " ['하여야'],\n",
       " ['비로소'],\n",
       " ['한다면 몰라도'],\n",
       " ['외에도'],\n",
       " ['이곳'],\n",
       " ['여기'],\n",
       " ['부터'],\n",
       " ['기점으로'],\n",
       " ['따라서'],\n",
       " ['할 생각이다'],\n",
       " ['하려고하다'],\n",
       " ['이리하여'],\n",
       " ['그리하여'],\n",
       " ['그렇게 함으로써'],\n",
       " ['하지만'],\n",
       " ['일때'],\n",
       " ['할때'],\n",
       " ['앞에서'],\n",
       " ['중에서'],\n",
       " ['보는데서'],\n",
       " ['으로써'],\n",
       " ['로써'],\n",
       " ['까지'],\n",
       " ['해야한다'],\n",
       " ['일것이다'],\n",
       " ['반드시'],\n",
       " ['할줄알다'],\n",
       " ['할수있다'],\n",
       " ['할수있어'],\n",
       " ['임에 틀림없다'],\n",
       " ['한다면'],\n",
       " ['등'],\n",
       " ['등등'],\n",
       " ['제'],\n",
       " ['겨우'],\n",
       " ['단지'],\n",
       " ['다만'],\n",
       " ['할뿐'],\n",
       " ['딩동'],\n",
       " ['댕그'],\n",
       " ['대해서'],\n",
       " ['대하여'],\n",
       " ['대하면'],\n",
       " ['훨씬'],\n",
       " ['얼마나'],\n",
       " ['얼마만큼'],\n",
       " ['얼마큼'],\n",
       " ['남짓'],\n",
       " ['여'],\n",
       " ['얼마간'],\n",
       " ['약간'],\n",
       " ['다소'],\n",
       " ['좀'],\n",
       " ['조금'],\n",
       " ['다수'],\n",
       " ['몇'],\n",
       " ['얼마'],\n",
       " ['지만'],\n",
       " ['하물며'],\n",
       " ['또한'],\n",
       " ['그러나'],\n",
       " ['그렇지만'],\n",
       " ['하지만'],\n",
       " ['이외에도'],\n",
       " ['대해 말하자면'],\n",
       " ['뿐이다'],\n",
       " ['다음에'],\n",
       " ['반대로'],\n",
       " ['반대로 말하자면'],\n",
       " ['이와 반대로'],\n",
       " ['바꾸어서 말하면'],\n",
       " ['바꾸어서 한다면'],\n",
       " ['만약'],\n",
       " ['그렇지않으면'],\n",
       " ['까악'],\n",
       " ['툭'],\n",
       " ['딱'],\n",
       " ['삐걱거리다'],\n",
       " ['보드득'],\n",
       " ['비걱거리다'],\n",
       " ['꽈당'],\n",
       " ['응당'],\n",
       " ['해야한다'],\n",
       " ['에 가서'],\n",
       " ['각'],\n",
       " ['각각'],\n",
       " ['여러분'],\n",
       " ['각종'],\n",
       " ['각자'],\n",
       " ['제각기'],\n",
       " ['하도록하다'],\n",
       " ['와'],\n",
       " ['과'],\n",
       " ['그러므로'],\n",
       " ['그래서'],\n",
       " ['고로'],\n",
       " ['한 까닭에'],\n",
       " ['하기 때문에'],\n",
       " ['거니와'],\n",
       " ['이지만'],\n",
       " ['대하여'],\n",
       " ['관하여'],\n",
       " ['관한'],\n",
       " ['과연'],\n",
       " ['실로'],\n",
       " ['아니나다를가'],\n",
       " ['생각한대로'],\n",
       " ['진짜로'],\n",
       " ['한적이있다'],\n",
       " ['하곤하였다'],\n",
       " ['하'],\n",
       " ['하하'],\n",
       " ['허허'],\n",
       " ['아하'],\n",
       " ['거바'],\n",
       " ['와'],\n",
       " ['오'],\n",
       " ['왜'],\n",
       " ['어째서'],\n",
       " ['무엇때문에'],\n",
       " ['어찌'],\n",
       " ['하겠는가'],\n",
       " ['무슨'],\n",
       " ['어디'],\n",
       " ['어느곳'],\n",
       " ['더군다나'],\n",
       " ['하물며'],\n",
       " ['더욱이는'],\n",
       " ['어느때'],\n",
       " ['언제'],\n",
       " ['야']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
    "stopwords[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a9ccd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def text_cleaning(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\n",
    "    result = hangul.sub('', text)\n",
    "    okt = Okt()  # 형태소 추출\n",
    "    nouns = okt.nouns(result)\n",
    "    nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거\n",
    "    nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거\n",
    "    return nouns\n",
    "\n",
    "vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\n",
    "bow_vect = vect.fit_transform(df['content'].tolist())\n",
    "word_list = vect.get_feature_names()\n",
    "count_list = bow_vect.toarray().sum(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "032624ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['개월',\n",
       " '귀염둥이',\n",
       " '꼬을',\n",
       " '남아',\n",
       " '다리',\n",
       " '단둘',\n",
       " '모습',\n",
       " '뽀뽀',\n",
       " '셀카',\n",
       " '소개',\n",
       " '스타벅스',\n",
       " '슬슬',\n",
       " '신경',\n",
       " '어린이',\n",
       " '오지',\n",
       " '일보',\n",
       " '자꾸',\n",
       " '제빵',\n",
       " '직전',\n",
       " '질질',\n",
       " '촬칵',\n",
       " '취임식',\n",
       " '트러블',\n",
       " '포스',\n",
       " '피부',\n",
       " '할머니']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afa71859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c45f2af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'개월': 1,\n",
       " '귀염둥이': 1,\n",
       " '꼬을': 1,\n",
       " '남아': 1,\n",
       " '다리': 1,\n",
       " '단둘': 1,\n",
       " '모습': 1,\n",
       " '뽀뽀': 1,\n",
       " '셀카': 1,\n",
       " '소개': 1,\n",
       " '스타벅스': 1,\n",
       " '슬슬': 1,\n",
       " '신경': 1,\n",
       " '어린이': 1,\n",
       " '오지': 1,\n",
       " '일보': 1,\n",
       " '자꾸': 1,\n",
       " '제빵': 1,\n",
       " '직전': 1,\n",
       " '질질': 1,\n",
       " '촬칵': 1,\n",
       " '취임식': 1,\n",
       " '트러블': 1,\n",
       " '포스': 1,\n",
       " '피부': 1,\n",
       " '할머니': 1}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_dict = dict(zip(word_list, count_list))\n",
    "word_count_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cff1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
